{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fáza 2 - Predspracovanie údajov\n",
    "\n",
    "__Autori:__ Dávid Penťa, Samuel Bernát\n",
    "__Percentuálny podiel práce:__ 50% / 50%\n",
    "\n",
    "V tejto fáze sa od Vás očakáva že realizujte predspracovanie údajov pre strojové učenie. Výsledkom bude upravená dátová sada (csv alebo tsv), kde jedno pozorovanie je opísané jedným riadkom.\n",
    "- scikit-learn vie len numerické dáta, takže treba niečo spraviť s nenumerickými dátami.\n",
    "- Replikovateľnosť predspracovania na trénovacej a testovacej množine dát, aby ste mohli\n",
    "zopakovať predspracovanie viackrát podľa Vašej potreby (iteratívne).\n",
    "\n",
    "Keď sa predspracovaním mohol zmeniť tvar a charakteristiky dát, je možné že treba realizovať EDA opakovane podľa Vašej potreby. Bodovanie znovu (EDA) nebudeme, zmeny ale dokumentujte. Problém s dátami môžete riešiť iteratívne v každej fáze aj vo všetkých fázach podľa potreby."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import dateparser as dateparser\n",
    "import matplotlib\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as py\n",
    "import statsmodels.stats.api as sms\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import pearsonr\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "from numpy import exp\n",
    "from numpy.random import randn\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from operator import itemgetter\n",
    "from sklearn.impute import KNNImputer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "measurements_file = \"data/measurements.csv\"\n",
    "measurements_data = pd.read_csv(measurements_file, sep='\\t')\n",
    "measurements_data_med = pd.read_csv(measurements_file, sep='\\t')\n",
    "measurements_data_avg = pd.read_csv(measurements_file, sep='\\t')\n",
    "measurements_data_knn = pd.read_csv(measurements_file, sep='\\t')\n",
    "measurements_data_out = pd.read_csv(measurements_file, sep='\\t')\n",
    "\n",
    "stations_file = \"data/stations.csv\"\n",
    "stations_data = pd.read_csv(stations_file, sep='\\t')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Integrácia a čistenie dát\n",
    "Transformujte dáta na vhodný formát pre strojové učenie t.j. jedno pozorovanie musí byť opísané jedným riadkom a každý atribút musí byť v numerickom formáte.\n",
    "- Pri riešení chýbajúcich hodnôt (missing values) vyskúšajte rôzne stratégie ako napr.\n",
    "    - odstránenie pozorovaní s chýbajúcimi údajmi\n",
    "    - nahradenie chýbajúcej hodnoty mediánom, priemerom, pomerom (ku korelovanému atribútu), alebo pomocou lineárnej regresie resp. kNN\n",
    "- Podobne postupujte aj pri riešení vychýlených hodnôt (outlier detection):\n",
    "    - odstránenie vychýlených (odľahlých) pozorovaní\n",
    "    - nahradenie vychýlenej hodnoty hraničnými hodnotami rozdelenia (5% resp. 95%)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Chýbajúce hodnoty\n",
    "Prvou použitou technikou odstránim pozorovania s chýbajúcimi údajmi (v tvare Nan)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11354 entries, 0 to 12067\n",
      "Data columns (total 18 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   PM10       11354 non-null  float64\n",
      " 1   CO         11354 non-null  float64\n",
      " 2   Pb         11354 non-null  float64\n",
      " 3   C2H3NO5    11354 non-null  float64\n",
      " 4   CFCs       11354 non-null  float64\n",
      " 5   H2CO       11354 non-null  float64\n",
      " 6   O3         11354 non-null  float64\n",
      " 7   TEMP       11354 non-null  float64\n",
      " 8   NOx        11354 non-null  float64\n",
      " 9   SO2        11354 non-null  float64\n",
      " 10  latitude   11354 non-null  float64\n",
      " 11  longitude  11354 non-null  float64\n",
      " 12  NH3        11354 non-null  float64\n",
      " 13  CH4        11354 non-null  float64\n",
      " 14  PRES       11354 non-null  float64\n",
      " 15  PM2.5      11354 non-null  float64\n",
      " 16  warning    11354 non-null  float64\n",
      " 17  PAHs       11354 non-null  float64\n",
      "dtypes: float64(18)\n",
      "memory usage: 1.6 MB\n"
     ]
    }
   ],
   "source": [
    "measurements_data.dropna(inplace=True)\n",
    "stations_data.dropna(inplace=True)\n",
    "\n",
    "stations_data[\"QoS\"] = np.where(stations_data[\"QoS\"] == \"accep\", \"acceptable\", stations_data[\"QoS\"])\n",
    "stations_data[\"QoS\"] = np.where(stations_data[\"QoS\"] == \"maitennce\", \"maintenance\", stations_data[\"QoS\"])\n",
    "\n",
    "# stations_data['revision'] = stations_data['revision'].apply(lambda x: pd.Timestamp(x).strftime('%B-%d-%Y'))\n",
    "# stations_data['revision_timestamp'] = stations_data['revision'].apply(lambda x: pd.Timestamp(x).timestamp())\n",
    "stations_data['revision'] = stations_data['revision'].apply(lambda x: pd.Timestamp(x).timestamp())\n",
    "\n",
    "stations_data['latitude'] = stations_data['latitude'].round(5)\n",
    "stations_data['longitude'] = stations_data['longitude'].round(5)\n",
    "\n",
    "stations_data[\"station\"] = np.where(stations_data[\"station\"] == \"T‚Äôaebaek\", \"Taebaek\", stations_data[\"station\"])\n",
    "stations_data[\"station\"] = np.where(stations_data[\"station\"] == \"'Ali Sabieh\", \"Ali Sabieh\", stations_data[\"station\"])\n",
    "stations_data[\"station\"] = np.where(stations_data[\"station\"] == \"Oktyabr‚Äôskiy\", \"Oktyabrsk\", stations_data[\"station\"])\n",
    "stations_data[\"station\"] = np.where(stations_data[\"station\"] == \"Roslavl‚Äô\", \"Roslavl\", stations_data[\"station\"])\n",
    "stations_data[\"station\"] = np.where(stations_data[\"station\"] == \"Dyat‚Äôkovo\", \"Dyatkovo\", stations_data[\"station\"])\n",
    "\n",
    "#stations_data\n",
    "measurements_data.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Druhou technikou nahradím v datasete chýbajúce hodnoty mediánom daného stĺpca."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "for col in measurements_data_med:\n",
    "    measurements_data_med[col].fillna((measurements_data[col].median()), inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Treťou technikou nahradím v datasete chýbajúce hodnoty priemerom daného stĺpca."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "for col in measurements_data_avg:\n",
    "    measurements_data_avg[col].fillna((measurements_data[col].mean()), inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Poslenou technikou nahradím v datasete chýbajúce hodnoty hodnotou vypočítanou metódou kNN, s nastavením na 5 susedov."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "data = measurements_data_knn.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 17]\n",
    "X, y = data[:, ix], data[:, 17]\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(X)\n",
    "Xtrans = imputer.transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Outlier detection\n",
    "Použitím prvej techniky odstránim vychýlené pozorovania z datasetu."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old:  (12068, 18)\n",
      "New:  (12068, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"Old: \", measurements_data.shape)\n",
    "for col in measurements_data_out:\n",
    "    Q1 = np.percentile(measurements_data_out[col], 25, method = 'midpoint')\n",
    "    Q3 = np.percentile(measurements_data_out[col], 75, method = 'midpoint')\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    upper = np.where(measurements_data_out[col] >= (Q3+1.5*IQR))\n",
    "    lower = np.where(measurements_data_out[col] <= (Q1-1.5*IQR))\n",
    "\n",
    "for i in range(len(upper)):\n",
    "    measurements_data_out.drop(upper[i], inplace = True)\n",
    "for j in range(len(lower)):\n",
    "    measurements_data_out.drop(lower[i], inplace = True)\n",
    "\n",
    "print(\"New: \", measurements_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Shape:  (506, 13)\n",
      "(array([], dtype=int64),) (array([351, 352, 353, 354, 355], dtype=int64),)\n",
      "New Shape:  (501, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samko\\Documents\\GitHub\\IAU\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\Samko\\AppData\\Local\\Temp\\ipykernel_2916\\3880602876.py:16: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  Q1 = np.percentile(df_boston['DIS'], 25,\n",
      "C:\\Users\\Samko\\AppData\\Local\\Temp\\ipykernel_2916\\3880602876.py:19: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  Q3 = np.percentile(df_boston['DIS'], 75,\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "bos_hou = load_boston()\n",
    "\n",
    "# Create the dataframe\n",
    "column_name = bos_hou.feature_names\n",
    "df_boston = pd.DataFrame(bos_hou.data)\n",
    "df_boston.columns = column_name\n",
    "df_boston.head()\n",
    "\n",
    "''' Detection '''\n",
    "# IQR\n",
    "Q1 = np.percentile(df_boston['DIS'], 25,\n",
    "                   interpolation = 'midpoint')\n",
    "\n",
    "Q3 = np.percentile(df_boston['DIS'], 75,\n",
    "                   interpolation = 'midpoint')\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(\"Old Shape: \", df_boston.shape)\n",
    "\n",
    "# Upper bound\n",
    "upper = np.where(df_boston['DIS'] >= (Q3+1.5*IQR))\n",
    "# Lower bound\n",
    "lower = np.where(df_boston['DIS'] <= (Q1-1.5*IQR))\n",
    "print(lower, upper)\n",
    "\n",
    "''' Removing the Outliers '''\n",
    "df_boston.drop(upper[0], inplace = True)\n",
    "df_boston.drop(lower[0], inplace = True)\n",
    "\n",
    "print(\"New Shape: \", df_boston.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stations_data = stations_data.groupby(by='station').apply(lambda x: x.loc[x['revision']==x['revision'].max()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spojenie tabuliek"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df=pd.merge(stations_data, measurements_data, on=['latitude', 'longitude'], how='inner')\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Opis každého atribútu numerickým formátom"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['QoS_ID'] = -1\n",
    "\n",
    "df.loc[df.QoS == \"excellent\", \"QoS_ID\"] = \"1\"\n",
    "df.loc[df.QoS == \"good\", \"QoS_ID\"] = \"2\"\n",
    "df.loc[df.QoS == \"average\", \"QoS_ID\"] = \"3\"\n",
    "df.loc[df.QoS == \"acceptable\", \"QoS_ID\"] = \"4\"\n",
    "df.loc[df.QoS == \"building\", \"QoS_ID\"] = \"5\"\n",
    "df.loc[df.QoS == \"maintenance\", \"QoS_ID\"] = \"5\"\n",
    "\n",
    "df[['QoS_ID']] = df[['QoS_ID']].apply(pd.to_numeric)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['station_ID'] = df['station'].rank(method='dense')\n",
    "df['code_ID'] = df['code'].rank(method='dense')\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Uloženie upravenej dátovej sady do .csv súboru"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output = df[['station_ID', 'code_ID', 'QoS_ID', 'warning', 'latitude', 'longitude', 'revision','PAHs', 'PM10', 'CO', 'Pb', 'C2H3NO5', 'CFCs', 'H2CO', 'O3', 'TEMP', 'NOx', 'SO2', 'NH3', 'CH4', 'PRES', 'PM2.5']]\n",
    "\n",
    "output.to_csv('output.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Power Transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def transform(column_name):\n",
    "\n",
    "    sns.histplot(data=df, hue='warning', x=column_name, fill=True, kde=True)\n",
    "    plt.show()\n",
    "\n",
    "    data = df[column_name].values\n",
    "    data = data.reshape((len(data),1))\n",
    "\n",
    "    power = PowerTransformer(\n",
    "                method='yeo-johnson',\n",
    "                standardize=True)\n",
    "    data_trans = power.fit_transform(data)\n",
    "    df[column_name] = data_trans\n",
    "\n",
    "    sns.histplot(data=df, hue='warning', x=column_name, fill=True, kde=True)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transform('C2H3NO5')\n",
    "\n",
    "# arr_0 = ['PAHs', 'PM10', 'CO', 'Pb', 'C2H3NO5', 'CFCs', 'H2CO', 'O3', 'TEMP', 'NOx', 'SO2', 'NH3', 'CH4', 'PRES', 'PM2.5']\n",
    "# for i in arr_0:\n",
    "#     print(i)\n",
    "#     transform(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Zoradenie podla p a zaroven r"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "arr = []\n",
    "#\n",
    "for i in list(df.columns.values):\n",
    "    if i != 'QoS' and i != 'station' and i != 'code':\n",
    "        (r, p) = pearsonr(df['warning'], df[i])\n",
    "        if p < 0.05 and i != 'warning':\n",
    "            arr.append([i, abs(r), p])\n",
    "\n",
    "arr = sorted(arr, key=itemgetter(1))\n",
    "a = []\n",
    "for i in range(len(arr)):\n",
    "    a.append([i + 1, arr[len(arr) - i - 1][0], arr[len(arr) - i - 1][1], arr[len(arr) - i - 1][2]])\n",
    "\n",
    "for row in a:\n",
    "    print(\"{: >0} {: >10} {:.5f} {:.5f}\".format(*row))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}